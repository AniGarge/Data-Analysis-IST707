{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "\n",
    "data=pd.read_csv(\"Disputed_Essay_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>filename</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dispt</td>\n",
       "      <td>dispt_fed_49.txt</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dispt</td>\n",
       "      <td>dispt_fed_50.txt</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dispt</td>\n",
       "      <td>dispt_fed_51.txt</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dispt</td>\n",
       "      <td>dispt_fed_52.txt</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dispt</td>\n",
       "      <td>dispt_fed_53.txt</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author          filename      a    all   also     an    and    any    are  \\\n",
       "0  dispt  dispt_fed_49.txt  0.280  0.052  0.009  0.096  0.358  0.026  0.131   \n",
       "1  dispt  dispt_fed_50.txt  0.177  0.063  0.013  0.038  0.393  0.063  0.051   \n",
       "2  dispt  dispt_fed_51.txt  0.339  0.090  0.008  0.030  0.301  0.008  0.068   \n",
       "3  dispt  dispt_fed_52.txt  0.270  0.024  0.016  0.024  0.262  0.056  0.064   \n",
       "4  dispt  dispt_fed_53.txt  0.303  0.054  0.027  0.034  0.404  0.040  0.128   \n",
       "\n",
       "      as  ...     was   were   what   when  which    who   will   with  would  \\\n",
       "0  0.122  ...   0.009  0.017  0.000  0.009  0.175  0.044  0.009  0.087  0.192   \n",
       "1  0.139  ...   0.051  0.000  0.000  0.000  0.114  0.038  0.089  0.063  0.139   \n",
       "2  0.203  ...   0.008  0.015  0.008  0.000  0.105  0.008  0.173  0.045  0.068   \n",
       "3  0.111  ...   0.087  0.079  0.008  0.024  0.167  0.000  0.079  0.079  0.064   \n",
       "4  0.148  ...   0.027  0.020  0.020  0.007  0.155  0.027  0.168  0.074  0.040   \n",
       "\n",
       "   your  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 72 columns]"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the data\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.293235</td>\n",
       "      <td>0.076799</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.052835</td>\n",
       "      <td>0.023633</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.007659</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.068388</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.384624</td>\n",
       "      <td>0.114493</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>any</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.041612</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.077071</td>\n",
       "      <td>0.034450</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.040605</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.044271</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.301153</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>been</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.059671</td>\n",
       "      <td>0.033131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.017720</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.127153</td>\n",
       "      <td>0.049303</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.035576</td>\n",
       "      <td>0.027954</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.006259</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009871</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.023906</td>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.093765</td>\n",
       "      <td>0.035143</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.079776</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.021165</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.044424</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.094741</td>\n",
       "      <td>0.037877</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>her</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>his</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.028624</td>\n",
       "      <td>0.042516</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.018064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.317353</td>\n",
       "      <td>0.071344</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.024094</td>\n",
       "      <td>0.019219</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.156271</td>\n",
       "      <td>0.058501</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.156741</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.022492</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>only</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.022882</td>\n",
       "      <td>0.014658</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>or</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.096741</td>\n",
       "      <td>0.045057</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>our</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.032833</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.018753</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>should</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.019757</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.029824</td>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>some</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.019894</td>\n",
       "      <td>0.014955</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.016636</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>than</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.043965</td>\n",
       "      <td>0.026781</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.212035</td>\n",
       "      <td>0.058551</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>85.0</td>\n",
       "      <td>1.280776</td>\n",
       "      <td>0.188238</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.275</td>\n",
       "      <td>1.423</td>\n",
       "      <td>1.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.085529</td>\n",
       "      <td>0.041628</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>then</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.006082</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.026376</td>\n",
       "      <td>0.022051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>things</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.002659</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.087012</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.535788</td>\n",
       "      <td>0.104227</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.006801</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upon</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.029224</td>\n",
       "      <td>0.027421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.029051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>0.020868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>when</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.011741</td>\n",
       "      <td>0.013254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>which</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.157753</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>who</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.024883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.098647</td>\n",
       "      <td>0.065930</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.079682</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.101718</td>\n",
       "      <td>0.074626</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>85.0</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std    min    25%    50%    75%    max\n",
       "a        85.0  0.293235  0.076799  0.096  0.240  0.299  0.349  0.466\n",
       "all      85.0  0.052835  0.023633  0.015  0.035  0.050  0.066  0.127\n",
       "also     85.0  0.007659  0.008409  0.000  0.000  0.007  0.013  0.047\n",
       "an       85.0  0.068388  0.029630  0.009  0.049  0.071  0.085  0.179\n",
       "and      85.0  0.384624  0.114493  0.217  0.319  0.358  0.413  0.821\n",
       "any      85.0  0.041612  0.022785  0.000  0.025  0.043  0.056  0.114\n",
       "are      85.0  0.077071  0.034450  0.013  0.051  0.068  0.102  0.163\n",
       "as       85.0  0.124200  0.040605  0.027  0.100  0.124  0.144  0.252\n",
       "at       85.0  0.044271  0.026210  0.000  0.026  0.038  0.063  0.118\n",
       "be       85.0  0.301153  0.086585  0.040  0.258  0.307  0.358  0.481\n",
       "been     85.0  0.059671  0.033131  0.000  0.030  0.053  0.084  0.165\n",
       "but      85.0  0.032318  0.017720  0.000  0.022  0.032  0.042  0.089\n",
       "by       85.0  0.127153  0.049303  0.027  0.092  0.124  0.162  0.264\n",
       "can      85.0  0.035576  0.027954  0.000  0.014  0.029  0.052  0.110\n",
       "do       85.0  0.006259  0.006569  0.000  0.000  0.006  0.010  0.028\n",
       "down     85.0  0.001529  0.003607  0.000  0.000  0.000  0.000  0.017\n",
       "even     85.0  0.011400  0.009871  0.000  0.000  0.010  0.018  0.037\n",
       "every    85.0  0.023906  0.018355  0.000  0.009  0.022  0.034  0.087\n",
       "for      85.0  0.093765  0.035143  0.030  0.070  0.088  0.114  0.213\n",
       "from     85.0  0.079776  0.027275  0.026  0.057  0.078  0.098  0.162\n",
       "had      85.0  0.021165  0.022510  0.000  0.008  0.016  0.027  0.141\n",
       "has      85.0  0.044424  0.024926  0.000  0.025  0.046  0.057  0.114\n",
       "have     85.0  0.094741  0.037877  0.011  0.073  0.090  0.124  0.185\n",
       "her      85.0  0.008094  0.021472  0.000  0.000  0.000  0.007  0.150\n",
       "his      85.0  0.028624  0.042516  0.000  0.000  0.014  0.039  0.247\n",
       "if       85.0  0.027329  0.018064  0.000  0.016  0.026  0.034  0.099\n",
       "in       85.0  0.317353  0.071344  0.189  0.267  0.304  0.355  0.499\n",
       "into     85.0  0.024094  0.019219  0.000  0.010  0.022  0.034  0.105\n",
       "is       85.0  0.156271  0.058501  0.028  0.118  0.151  0.196  0.323\n",
       "it       85.0  0.156741  0.044831  0.075  0.129  0.151  0.190  0.284\n",
       "...       ...       ...       ...    ...    ...    ...    ...    ...\n",
       "one      85.0  0.040788  0.022492  0.005  0.027  0.036  0.050  0.135\n",
       "only     85.0  0.022882  0.014658  0.000  0.010  0.022  0.034  0.065\n",
       "or       85.0  0.096741  0.045057  0.027  0.070  0.081  0.116  0.321\n",
       "our      85.0  0.023000  0.032833  0.000  0.000  0.013  0.028  0.199\n",
       "shall    85.0  0.018753  0.019245  0.000  0.006  0.014  0.027  0.079\n",
       "should   85.0  0.026565  0.019757  0.000  0.010  0.027  0.038  0.091\n",
       "so       85.0  0.029824  0.016065  0.000  0.018  0.029  0.040  0.072\n",
       "some     85.0  0.019894  0.014955  0.000  0.009  0.017  0.028  0.067\n",
       "such     85.0  0.029224  0.016636  0.000  0.018  0.029  0.038  0.085\n",
       "than     85.0  0.043965  0.026781  0.000  0.027  0.043  0.055  0.150\n",
       "that     85.0  0.212035  0.058551  0.081  0.171  0.208  0.244  0.380\n",
       "the      85.0  1.280776  0.188238  0.669  1.178  1.275  1.423  1.803\n",
       "their    85.0  0.085529  0.041628  0.005  0.055  0.086  0.106  0.183\n",
       "then     85.0  0.006082  0.006077  0.000  0.000  0.006  0.010  0.021\n",
       "there    85.0  0.026376  0.022051  0.000  0.009  0.022  0.039  0.105\n",
       "things   85.0  0.002659  0.004382  0.000  0.000  0.000  0.006  0.015\n",
       "this     85.0  0.087012  0.029026  0.009  0.069  0.090  0.105  0.153\n",
       "to       85.0  0.535788  0.104227  0.333  0.469  0.540  0.606  0.776\n",
       "up       85.0  0.003482  0.006801  0.000  0.000  0.000  0.006  0.032\n",
       "upon     85.0  0.029224  0.027421  0.000  0.000  0.028  0.050  0.102\n",
       "was      85.0  0.025835  0.029051  0.000  0.009  0.015  0.032  0.189\n",
       "were     85.0  0.020224  0.020868  0.000  0.007  0.015  0.029  0.108\n",
       "what     85.0  0.012859  0.011663  0.000  0.005  0.010  0.020  0.060\n",
       "when     85.0  0.011741  0.013254  0.000  0.000  0.009  0.015  0.073\n",
       "which    85.0  0.157753  0.043046  0.081  0.118  0.152  0.183  0.276\n",
       "who      85.0  0.032529  0.024883  0.000  0.016  0.027  0.044  0.129\n",
       "will     85.0  0.098647  0.065930  0.006  0.052  0.081  0.135  0.340\n",
       "with     85.0  0.079682  0.025680  0.027  0.061  0.079  0.092  0.150\n",
       "would    85.0  0.101718  0.074626  0.009  0.042  0.078  0.147  0.382\n",
       "your     85.0  0.002024  0.009693  0.000  0.000  0.000  0.000  0.074\n",
       "\n",
       "[70 rows x 8 columns]"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Statistics for the given data.\n",
    "\n",
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author      0\n",
       "filename    0\n",
       "a           0\n",
       "all         0\n",
       "also        0\n",
       "an          0\n",
       "and         0\n",
       "any         0\n",
       "are         0\n",
       "as          0\n",
       "at          0\n",
       "be          0\n",
       "been        0\n",
       "but         0\n",
       "by          0\n",
       "can         0\n",
       "do          0\n",
       "down        0\n",
       "even        0\n",
       "every       0\n",
       "for         0\n",
       "from        0\n",
       "had         0\n",
       "has         0\n",
       "have        0\n",
       "her         0\n",
       "his         0\n",
       "if          0\n",
       "in          0\n",
       "into        0\n",
       "           ..\n",
       "one         0\n",
       "only        0\n",
       "or          0\n",
       "our         0\n",
       "shall       0\n",
       "should      0\n",
       "so          0\n",
       "some        0\n",
       "such        0\n",
       "than        0\n",
       "that        0\n",
       "the         0\n",
       "their       0\n",
       "then        0\n",
       "there       0\n",
       "things      0\n",
       "this        0\n",
       "to          0\n",
       "up          0\n",
       "upon        0\n",
       "was         0\n",
       "were        0\n",
       "what        0\n",
       "when        0\n",
       "which       0\n",
       "who         0\n",
       "will        0\n",
       "with        0\n",
       "would       0\n",
       "your        0\n",
       "Length: 72, dtype: int64"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total nulls in every column.\n",
    "\n",
    "data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total nulls in the data\n",
    "\n",
    "sum(data.isnull().values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dispt</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dispt</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dispt</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dispt</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dispt</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  author      a    all   also     an    and    any    are     as     at  ...   \\\n",
       "0  dispt  0.280  0.052  0.009  0.096  0.358  0.026  0.131  0.122  0.017  ...    \n",
       "1  dispt  0.177  0.063  0.013  0.038  0.393  0.063  0.051  0.139  0.114  ...    \n",
       "2  dispt  0.339  0.090  0.008  0.030  0.301  0.008  0.068  0.203  0.023  ...    \n",
       "3  dispt  0.270  0.024  0.016  0.024  0.262  0.056  0.064  0.111  0.056  ...    \n",
       "4  dispt  0.303  0.054  0.027  0.034  0.404  0.040  0.128  0.148  0.013  ...    \n",
       "\n",
       "     was   were   what   when  which    who   will   with  would  your  \n",
       "0  0.009  0.017  0.000  0.009  0.175  0.044  0.009  0.087  0.192   0.0  \n",
       "1  0.051  0.000  0.000  0.000  0.114  0.038  0.089  0.063  0.139   0.0  \n",
       "2  0.008  0.015  0.008  0.000  0.105  0.008  0.173  0.045  0.068   0.0  \n",
       "3  0.087  0.079  0.008  0.024  0.167  0.000  0.079  0.079  0.064   0.0  \n",
       "4  0.027  0.020  0.020  0.007  0.155  0.027  0.168  0.074  0.040   0.0  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We already have the author name, so we drop the column 'filename'.\n",
    "\n",
    "data=data.drop('filename',axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 'HM' with 'Hamilton'\n",
    "\n",
    "data['author']=data['author'].replace('HM','Hamilton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the test and train data according to problem statement.\n",
    "\n",
    "disputed_data=data[data['author']=='dispt']\n",
    "non_disputed_data=data[data['author']!='dispt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>be</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.280</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.177</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.339</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       a    all   also     an    and    any    are     as     at     be  ...   \\\n",
       "0  0.280  0.052  0.009  0.096  0.358  0.026  0.131  0.122  0.017  0.411  ...    \n",
       "1  0.177  0.063  0.013  0.038  0.393  0.063  0.051  0.139  0.114  0.393  ...    \n",
       "2  0.339  0.090  0.008  0.030  0.301  0.008  0.068  0.203  0.023  0.474  ...    \n",
       "3  0.270  0.024  0.016  0.024  0.262  0.056  0.064  0.111  0.056  0.365  ...    \n",
       "4  0.303  0.054  0.027  0.034  0.404  0.040  0.128  0.148  0.013  0.344  ...    \n",
       "\n",
       "     was   were   what   when  which    who   will   with  would  your  \n",
       "0  0.009  0.017  0.000  0.009  0.175  0.044  0.009  0.087  0.192   0.0  \n",
       "1  0.051  0.000  0.000  0.000  0.114  0.038  0.089  0.063  0.139   0.0  \n",
       "2  0.008  0.015  0.008  0.000  0.105  0.008  0.173  0.045  0.068   0.0  \n",
       "3  0.087  0.079  0.008  0.024  0.167  0.000  0.079  0.079  0.064   0.0  \n",
       "4  0.027  0.020  0.020  0.007  0.155  0.027  0.168  0.074  0.040   0.0  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the first column because it is a categorical variable.\n",
    "\n",
    "disputed_data=disputed_data.iloc[:,1:]\n",
    "disputed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.115</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      author      a    all   also     an    and    any    are     as     at  \\\n",
       "11  Hamilton  0.213  0.083  0.000  0.083  0.343  0.056  0.111  0.093  0.065   \n",
       "12  Hamilton  0.369  0.070  0.006  0.076  0.411  0.023  0.053  0.117  0.065   \n",
       "13  Hamilton  0.305  0.047  0.007  0.068  0.386  0.047  0.102  0.108  0.088   \n",
       "14  Hamilton  0.391  0.045  0.015  0.030  0.270  0.045  0.060  0.090  0.015   \n",
       "15  Hamilton  0.327  0.096  0.000  0.086  0.356  0.014  0.086  0.072  0.115   \n",
       "\n",
       "    ...      was   were   what   when  which    who   will   with  would  \\\n",
       "11  ...    0.000  0.000  0.000  0.009  0.158  0.074  0.222  0.046  0.019   \n",
       "12  ...    0.000  0.012  0.012  0.012  0.147  0.029  0.094  0.129  0.270   \n",
       "13  ...    0.000  0.000  0.007  0.000  0.156  0.007  0.074  0.122  0.149   \n",
       "14  ...    0.000  0.000  0.000  0.045  0.165  0.045  0.135  0.150  0.210   \n",
       "15  ...    0.014  0.038  0.014  0.019  0.264  0.029  0.091  0.086  0.062   \n",
       "\n",
       "     your  \n",
       "11  0.074  \n",
       "12  0.000  \n",
       "13  0.000  \n",
       "14  0.000  \n",
       "15  0.010  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_disputed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries for the decision tree model and clustering model.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the 'author' column as train_Y and remaining columns as train_X\n",
    "\n",
    "Y=non_disputed_data['author']\n",
    "X=non_disputed_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree with default settings\n",
    "\n",
    "clf=DecisionTreeClassifier()\n",
    "clf=clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hamilton', 'Madison', 'Hamilton', 'Jay', 'Madison', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Jay', 'Madison', 'Madison',\n",
       "       'Madison'], dtype=object)"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction for the test set to get all the accuracy metrics\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.26086956521739 %\n",
      "Precision is 0.7608695652173914\n",
      "Recall is 0.782608695652174\n",
      "F1 measure is  0.7707509881422925\n"
     ]
    }
   ],
   "source": [
    "# All the accuracy metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100,'%')\n",
    "print('Precision is',precision_score(y_test,y_pred,average='weighted'))\n",
    "print('Recall is',recall_score(y_test,y_pred,average='weighted'))\n",
    "print('F1 measure is ',f1_score(y_test,y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13157\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ['gini', 'entropy'], 'max_depth': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameter Tuning using thr Grid Search technique to identify the optimal parameters with 10 fold cross validation.\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = { 'criterion':['gini','entropy'],'max_depth': np.arange(1, 20)}\n",
    "dtree_gscv = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=10)\n",
    "dtree_gscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 14}"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output the best parameters from the Grid Search\n",
    "\n",
    "dtree_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hamilton', 'Jay', 'Hamilton', 'Madison', 'Madison', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton', 'Hamilton',\n",
       "       'Hamilton', 'Hamilton', 'Hamilton', 'Madison', 'Madison',\n",
       "       'Madison', 'Madison'], dtype=object)"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test set to measure the accuracy of the model from hyperparameter tuning.\n",
    "\n",
    "y_pred_test=dtree_gscv.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.30434782608695 %\n",
      "Precision is 0.9378881987577641\n",
      "Recall is 0.9130434782608695\n",
      "F1 measure is  0.898550724637681\n"
     ]
    }
   ],
   "source": [
    "# All the accuracy metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_final_dtree)*100,'%')\n",
    "print('Precision is',precision_score(y_test,y_pred_final_dtree,average='weighted'))\n",
    "print('Recall is',recall_score(y_test,y_pred_final_dtree,average='weighted'))\n",
    "print('F1 measure is ',f1_score(y_test,y_pred_final_dtree,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After hyperparameter tuning we see that our accuracy metrics have definitely increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Madison', 'Madison', 'Madison', 'Madison', 'Madison', 'Madison',\n",
       "       'Jay', 'Madison', 'Madison', 'Madison', 'Madison'], dtype=object)"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now predicting for the disputed data\n",
    "\n",
    "y_final_pred_dtree=dtree_gscv.predict(disputed_data)\n",
    "y_final_pred_dtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus, we are sure that all the disputed articles are written by Madison according to Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We remove the author column as clustering does not work on categorical variables.\n",
    "K_data=data.iloc[:,1:]\n",
    "\n",
    "# Standardizing the data\n",
    "K_data=StandardScaler().fit_transform(K_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 6, 6, 6, 6, 1, 3, 1, 6, 6, 3, 7, 7, 1, 7, 2, 7, 2, 7, 2, 7,\n",
       "       7, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 7, 2, 0, 0, 2, 7, 0, 2, 7,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 7, 2, 2, 2, 2, 2, 2, 7, 4, 4, 4, 5,\n",
       "       5, 5, 5, 5, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 3])"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default K means Model\n",
    "\n",
    "kmeans=KMeans().fit(K_data)\n",
    "kmeans.predict(K_data)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85,)"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we have fit the model on whole data we see if the labels matches the dimension and yes it does.\n",
    "\n",
    "kmeans.labels_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Madison</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     author      a    all   also     an    and    any    are     as     at  \\\n",
       "70  Madison  0.359  0.020  0.000  0.069  0.590  0.020  0.133  0.093  0.039   \n",
       "71  Madison  0.221  0.034  0.007  0.062  0.407  0.021  0.048  0.159  0.048   \n",
       "72  Madison  0.290  0.048  0.016  0.048  0.523  0.011  0.095  0.148  0.011   \n",
       "73  Madison  0.378  0.032  0.014  0.072  0.401  0.041  0.081  0.126  0.027   \n",
       "74  Madison  0.300  0.074  0.006  0.034  0.346  0.023  0.113  0.187  0.040   \n",
       "75  Madison  0.250  0.043  0.014  0.058  0.461  0.010  0.062  0.144  0.034   \n",
       "76  Madison  0.302  0.063  0.004  0.084  0.369  0.025  0.063  0.130  0.021   \n",
       "77  Madison  0.240  0.042  0.000  0.078  0.491  0.057  0.047  0.115  0.016   \n",
       "78  Madison  0.380  0.051  0.013  0.051  0.333  0.034  0.064  0.154  0.026   \n",
       "79  Madison  0.212  0.101  0.010  0.051  0.440  0.076  0.066  0.136  0.020   \n",
       "80  Madison  0.136  0.054  0.014  0.048  0.422  0.027  0.048  0.150  0.027   \n",
       "81  Madison  0.212  0.028  0.006  0.050  0.391  0.033  0.073  0.117  0.033   \n",
       "82  Madison  0.177  0.052  0.047  0.047  0.436  0.026  0.135  0.083  0.036   \n",
       "83  Madison  0.243  0.091  0.008  0.084  0.372  0.008  0.046  0.137  0.030   \n",
       "84  Madison  0.347  0.097  0.007  0.056  0.313  0.035  0.049  0.132  0.035   \n",
       "\n",
       "    ...      was   were   what   when  which    who   will   with  would  \\\n",
       "70  ...    0.010  0.000  0.025  0.005  0.192  0.044  0.147  0.049  0.029   \n",
       "71  ...    0.021  0.014  0.014  0.000  0.276  0.007  0.179  0.048  0.034   \n",
       "72  ...    0.011  0.016  0.005  0.005  0.232  0.016  0.053  0.084  0.032   \n",
       "73  ...    0.054  0.041  0.000  0.009  0.117  0.063  0.054  0.090  0.063   \n",
       "74  ...    0.011  0.011  0.006  0.006  0.153  0.000  0.085  0.068  0.045   \n",
       "75  ...    0.058  0.091  0.019  0.014  0.139  0.043  0.024  0.072  0.029   \n",
       "76  ...    0.013  0.021  0.017  0.004  0.151  0.034  0.105  0.063  0.038   \n",
       "77  ...    0.016  0.016  0.005  0.010  0.141  0.021  0.031  0.083  0.078   \n",
       "78  ...    0.034  0.004  0.026  0.013  0.145  0.021  0.081  0.068  0.051   \n",
       "79  ...    0.020  0.010  0.010  0.000  0.152  0.010  0.051  0.091  0.147   \n",
       "80  ...    0.020  0.027  0.007  0.000  0.116  0.007  0.218  0.102  0.075   \n",
       "81  ...    0.067  0.011  0.022  0.000  0.128  0.028  0.223  0.095  0.162   \n",
       "82  ...    0.021  0.021  0.010  0.010  0.114  0.031  0.016  0.099  0.021   \n",
       "83  ...    0.023  0.023  0.008  0.000  0.213  0.038  0.076  0.061  0.023   \n",
       "84  ...    0.007  0.007  0.000  0.014  0.188  0.035  0.257  0.083  0.083   \n",
       "\n",
       "     your  \n",
       "70  0.000  \n",
       "71  0.034  \n",
       "72  0.000  \n",
       "73  0.000  \n",
       "74  0.000  \n",
       "75  0.000  \n",
       "76  0.000  \n",
       "77  0.000  \n",
       "78  0.000  \n",
       "79  0.000  \n",
       "80  0.000  \n",
       "81  0.000  \n",
       "82  0.000  \n",
       "83  0.000  \n",
       "84  0.000  \n",
       "\n",
       "[15 rows x 71 columns]"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have trained the K Means model on the whole data and we can see that disputed papers are in the cluster number 2 which also is the same cluster as the Madison authored paper, so we can conclude that disputed papers also belong to Madison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the elbow curve to determine optimal number of clusters.\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "distortions = []\n",
    "K = range(1, 11)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(K_data)\n",
    "    distortions.append(sum(np.min(cdist(K_data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / K_data.shape[0])\n",
    "    \n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method to Identify Optimal K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We know about the elbow curve in clustering techniques where as we increase number of clusters we get a higher performance, thus by the elbow curve I chose number of clusters as 4 which gives a solution to the problem statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the goal is obtained that Madison is the author of disputed papers, but now we need to reduce the misclassification and every author falls under a different cluster so that every disputed article falls under the same cluster as Madison, which is why we perform hyperaparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 1, 1,\n",
       "       3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Grid Search technique takes a long time to provide the optimal parameters so I used hit and trial to asess the\n",
    "# hyperparameters.\n",
    "\n",
    "kmeans=KMeans(n_clusters=4,n_init=25,max_iter=45).fit(K_data)\n",
    "kmeans.predict(K_data)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that every disputed article falls in same cluster as Madison and there is less missclassification than the previous KMeans model which is the result of the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that after implementing both the decision tree and clustering we get the results that disputed papers belong to Madison. But after hyperparameter tuning we get consistent results in both cases. Also, this is a case of unsupervised learning where we do not have the actual labels or the authors for the disputed papers. We rely just on the previous data given and perform the classification task in hand. Both the models use different ways to classify data but because accuracy metrics in case of decision trees is high we can trust the model and clustering groups all the similar items together in terms of the distance we can trust the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
